{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7eab823",
   "metadata": {},
   "source": [
    "# Steam AU Reviews & Items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78f879a",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e8cbf6",
   "metadata": {},
   "source": [
    "### Predictive task\n",
    "***Given a user's past sequence of games, what is the next game they buy?***\n",
    "\n",
    "The **input** features for our model include:\n",
    "* Hours played\n",
    "* Number of sessions\n",
    "* Game genre\n",
    "* Review text\n",
    "* Basic user history\n",
    "\n",
    "Note: Review text refers to processed user reviews through TF-IDF vectorization and analyzing sentiment scores. Basic user history refers to a user's past recommendation rate and what games already exist in their Steam library. \n",
    "\n",
    "The **output** of our model is a binary label (1 - recommend, 0 - not recommend) indicating whether the user recommends the game or not. This task is appropriate for supervised learning and aligns directly with models covered in the course."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759e8ad9",
   "metadata": {},
   "source": [
    "### Plans: Baselines and Evaluation\n",
    "\n",
    "We plan to use the following baseline models:\n",
    "* Random baseline\n",
    "* Logistic regression\n",
    "* Naive Bayes\n",
    "\n",
    "We plan to evaluate these models by comparing these metrics:\n",
    "* Accuracy\n",
    "* F1 score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177fbf19",
   "metadata": {},
   "source": [
    "## Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ac65ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data\n",
    "import gzip\n",
    "import ast\n",
    "from pathlib import Path\n",
    "\n",
    "# Essentials\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# Preprocessing & Splitting\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Pipelines & Feature Combos\n",
    "from sklearn.pipeline import make_pipeline, FeatureUnion\n",
    "\n",
    "# Models\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d01d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path('data')\n",
    "\n",
    "def load_python_dicts_gz(path: Path, max_rows=None, verbose=True) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    with gzip.open(path, 'rt', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f, start=1):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            rows.append(ast.literal_eval(line))\n",
    "            if max_rows is not None and len(rows) >= max_rows:\n",
    "                break\n",
    "            if verbose and i % 100_000 == 0:\n",
    "                print(f\"Read {i} lines from {path.name}...\")\n",
    "\n",
    "    df = pd.json_normalize(rows)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530c8fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load user reviews data\n",
    "reviews_path = DATA_DIR / 'australian_user_reviews.json.gz'\n",
    "reviews = load_python_dicts_gz(reviews_path, max_rows=100_000)\n",
    "\n",
    "print('reviews shape:', reviews.shape)\n",
    "print('reviews columns:')\n",
    "print(list(reviews.columns))\n",
    "\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7b0236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load user items data\n",
    "items_path = DATA_DIR / 'australian_users_items.json.gz'\n",
    "items = load_python_dicts_gz(items_path, max_rows=100)\n",
    "\n",
    "print('items shape:', items.shape)\n",
    "print('items columns:')\n",
    "print(list(items.columns))\n",
    "\n",
    "items.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c22759a",
   "metadata": {},
   "source": [
    "## Preprocessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceeb1fa1",
   "metadata": {},
   "source": [
    "### What does each value in the reviews column represent in the ``reviews`` dataframe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9288a7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "review_list = reviews['reviews'].iloc[0]\n",
    "print(type(review_list))\n",
    "\n",
    "pprint(review_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c071e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "first_review = review_list[0]\n",
    "pprint(first_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42638a16",
   "metadata": {},
   "source": [
    "### What about the items column in the ``items`` dataframe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d67491",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_list = items['items'].iloc[0]\n",
    "print(type(item_list))\n",
    "\n",
    "item_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb0ba88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "first_item = item_list[0]\n",
    "pprint(first_item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888e2a7f",
   "metadata": {},
   "source": [
    "It seems that each value in the reviews column and items column are both lists of dictionaries. In the reviews column, each value is a dictionary of reviews for different games. In the items column, each value is a dictionary of items, or games, for different Steam users.\n",
    "\n",
    "It's a bit hard to read each review from a user, especially if they leave a lot of reviews. We can explode the list of reviews to make it easier to read. We will also do the same thing for items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bb26a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Explode list of reviews so each review gets its own row\n",
    "reviews_long = reviews.explode('reviews').reset_index(drop = True)\n",
    "\n",
    "# Split each dictionary key into separate columns\n",
    "review_details = pd.json_normalize(reviews_long['reviews'])\n",
    "\n",
    "reviews_long = pd.concat(\n",
    "    [reviews_long.drop(columns = ['reviews']), review_details],\n",
    "    axis = 1\n",
    ")\n",
    "\n",
    "reviews_long.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ae02e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_long = items.explode(\"items\").reset_index(drop=True)\n",
    "\n",
    "item_details = pd.json_normalize(items_long[\"items\"])\n",
    "items_long = pd.concat(\n",
    "    [items_long.drop(columns=[\"items\"]), item_details],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "items_long.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309b188e",
   "metadata": {},
   "source": [
    "### Explaratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de57fe8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"reviews_long shape:\", reviews_long.shape)\n",
    "print(\"items_long shape:\", items_long.shape)\n",
    "\n",
    "print(\"Unique users in reviews:\", reviews_long[\"user_id\"].nunique())\n",
    "print(\"Unique users in items:\",   items_long[\"user_id\"].nunique())\n",
    "print(\"Unique items (reviews):\", reviews_long[\"item_id\"].nunique())\n",
    "print(\"Unique items (items):\",   items_long[\"item_id\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b62674",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_per_user = reviews_long.groupby(\"user_id\")[\"item_id\"].nunique()\n",
    "reviews_per_user.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69784c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "owned_per_user = items_long.groupby(\"user_id\")[\"item_id\"].nunique()\n",
    "owned_per_user.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196b9b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_long[\"playtime_forever\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c92e213",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_long[\"recommend\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf55364",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# keep only rows with an item_id\n",
    "reviews_long = reviews_long.dropna(subset=[\"item_id\"])\n",
    "\n",
    "# parse 'posted' like \"Posted November 5, 2011.\"\n",
    "posted_clean = (\n",
    "    reviews_long[\"posted\"]\n",
    "      .str.replace(\"Posted \", \"\", regex=False)\n",
    "      .str.rstrip(\".\")\n",
    ")\n",
    "\n",
    "reviews_long[\"timestamp\"] = pd.to_datetime(posted_clean, errors=\"coerce\")\n",
    "\n",
    "# drop rows we couldn't parse\n",
    "reviews_long = reviews_long.dropna(subset=[\"timestamp\"])\n",
    "\n",
    "# sort by time per user\n",
    "reviews_long = reviews_long.sort_values([\"user_id\", \"timestamp\"])\n",
    "reviews_long.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcae2b0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# make sure item_id is the same type in both tables\n",
    "reviews_long[\"item_id\"] = reviews_long[\"item_id\"].astype(str)\n",
    "items_long[\"item_id\"]   = items_long[\"item_id\"].astype(str)\n",
    "\n",
    "interactions = reviews_long.merge(\n",
    "    items_long[[\"user_id\", \"item_id\", \"item_name\", \"playtime_forever\", \"playtime_2weeks\"]],\n",
    "    on=[\"user_id\", \"item_id\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "interactions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2fb30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    interactions.groupby(\"recommend\")[\"playtime_forever\"]\n",
    "    .describe()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c3cfdd",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9929e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for TF-IDF\n",
    "\n",
    "features = FeatureUnion([\n",
    "    (\"word_tfidf\",\n",
    "     TfidfVectorizer(\n",
    "         max_features=50000,\n",
    "         ngram_range=(1,2),\n",
    "         min_df=3,\n",
    "         stop_words=\"english\",\n",
    "         sublinear_tf=True\n",
    "     )),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534b666c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    interactions[\"review\"],\n",
    "    interactions[\"recommend\"],\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360a2ff2",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c4ffe0",
   "metadata": {},
   "source": [
    "### Baseline models\n",
    "\n",
    "**1) Random baseline**\n",
    "\n",
    "Our first model is the random baseline, which is used to randomly predict either 0 or 1 based on class distribution. In the context of our dataset, it would be whether the user buys a game or not. As it is unpredictable, it is harder to beat than a majority-class baseline where there is an imbalance between buying and not buying a game, but it also ensures that our models will outperform randomness. This model also shows the value of actual machine learning models more clearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2379f377",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_baseline():\n",
    "    baseline = DummyClassifier(strategy=\"stratified\")\n",
    "    baseline.fit(X_train, y_train)\n",
    "    return baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd971646",
   "metadata": {},
   "source": [
    "**2) Logistic regression**\n",
    "\n",
    "Our next model is logistic regression&mdash;a strong and widely used baseline in machine learning. We chose this baseline since it works well with high-dimensional sparse features like TF-IDF and it is also simply, interpretable, and fast to train. For our dataset, logistic regression will learn to identify a weighted linear boundary between recommend or not recommend. Its weights also correspond directly to influential words since it is able to capture the direction and strength of sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c44e44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_logistic_regression():\n",
    "    model = make_pipeline(\n",
    "        features,\n",
    "        LogisticRegression(max_iter=2000)\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c099aac",
   "metadata": {},
   "source": [
    "**3) Naive Bayes**\n",
    "\n",
    "Why we included Naive Bayes\n",
    "\n",
    "- Classic baseline for text classification tasks.\n",
    "\n",
    "- Very fast to train and evaluate.\n",
    "\n",
    "- Performs surprisingly well on short reviews and simple sentiment.\n",
    "\n",
    "- Helps us check whether TF-IDF alone can produce strong performance.\n",
    "\n",
    "What it does\n",
    "\n",
    "- Uses word frequencies under a conditional independence assumption.\n",
    "\n",
    "- Learns how often words appear in positive vs. negative reviews.\n",
    "\n",
    "- Provides a lightweight benchmark to compare against more complex models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a9bbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_naive_bayes():\n",
    "    text_vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "\n",
    "    nb_model = make_pipeline(\n",
    "        text_vectorizer,\n",
    "        MultinomialNB()\n",
    "    )\n",
    "    nb_model.fit(X_train[\"review\"], y_train)\n",
    "    return nb_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fff026c",
   "metadata": {},
   "source": [
    "### Final model\n",
    "\n",
    "**Large TF-IDF + Linear SVC**\n",
    "\n",
    "Why LinearSVC?\n",
    "\n",
    "We choose a Linear Support Vector Classifier (LinearSVC) as our final model because:\n",
    "\n",
    "- It performs extremely well on high-dimensional sparse text data\n",
    "\n",
    "- It is more robust than Naive Bayes when features correlate\n",
    "\n",
    "- It scales better than kernel SVM for large datasets\n",
    "\n",
    "- It is fast to train on tens of thousands of TF-IDF features\n",
    "\n",
    "- It handles class imbalance well when paired with strong features\n",
    "\n",
    "Why combine multiple TF-IDF representations?\n",
    "\n",
    "- Our FeatureUnion merges different types of text signals:\n",
    "\n",
    "Word-level TF-IDF (1–2 grams)\n",
    "\n",
    "- captures phrases like “very fun”, “not good”\n",
    "\n",
    "Character-level TF-IDF (3–5 grams)\n",
    "\n",
    "- captures subword patterns\n",
    "\n",
    "- helps with misspellings, slang, repeated letters (“goooood”, “amazzing”)\n",
    "\n",
    "- helps stylized writing common in game reviews\n",
    "\n",
    "Together, these create a richer and more expressive representation of Steam review text.\n",
    "\n",
    "Why C=1.0?\n",
    "\n",
    "- A balanced default that prevents overfitting\n",
    "\n",
    "- Strong performance without needing heavy tuning\n",
    "\n",
    "Why Pipeline?\n",
    "\n",
    "Using make_pipeline ensures:\n",
    "\n",
    "- preprocessing + model are connected\n",
    "\n",
    "- no manual feature handling needed\n",
    "\n",
    "- one unified model object for training + prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57fba5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_linear_svc():\n",
    "    model = make_pipeline(\n",
    "        features,\n",
    "        LinearSVC(C=1.0)\n",
    "    )\n",
    "\n",
    "    print(\"Training final LinearSVC model...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00df3330",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Recall that our goal is to correctly predict whether a Steam user will recommend a game based on their review and gameplay behavior. Since this is a binary classification task, our evaluation needs to measure how well our model distinguishes between positive and negative recommendations.\n",
    "\n",
    "As mentioned in the beginning of the notebook, our plans were to use the following metrics to evaluate our models:\n",
    "\n",
    "* Accuracy\n",
    "\n",
    "Measuring accuracy will allow us to see the percentage of correct predictions our model makes, which is useful when classes (recommend vs. not recommend) are fairly balanced.\n",
    "\n",
    "* F1 Score\n",
    "\n",
    "The F1 score represents the harmonic mean for precision and recall. This value is more robust than accuracy if our dataset is imbalanced, which can apply to Steam reviews.\n",
    "\n",
    "We chose these metrics in particular due to the following reasons:\n",
    "* Recommendation data often contains more positive reviews than negative ones\n",
    "* Accuracy alone, especially at a high value, may be misleading for models that ignore the minority class\n",
    "* F1 score captures how well the model handles both sides, making it the most appropriate metric for this type of task\n",
    "\n",
    "Moving forward, we will be evaluating all of our models (baseline + final model) using the following helper function. In the end, we will compare our metrics in a table together to see which one scores higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615e2d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, name):\n",
    "    '''\n",
    "    Purpose: Keeps code clean and readable\n",
    "             Ensures all models are evaluated consistently\n",
    "    Outputs: accuracy, F1 score, ROC-AUC\n",
    "    '''\n",
    "    preds = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    f1 = f1_score(y_test, preds)\n",
    "\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(\"Accuracy:\", acc)\n",
    "    print(\"F1 Score:\", f1)\n",
    "\n",
    "    return {\n",
    "        \"model\": name,\n",
    "        \"accuracy\": acc,\n",
    "        \"f1\": f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14780cde",
   "metadata": {},
   "source": [
    "### Baseline evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d6d74d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "baseline = DummyClassifier(strategy=\"stratified\")\n",
    "baseline.fit(X_train, y_train)\n",
    "\n",
    "baseline_results = evaluate(baseline, \"Baseline: Random (Stratified)\")\n",
    "baseline_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2ab227",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_model = make_pipeline(TfidfVectorizer(), LogisticRegression(max_iter=300))\n",
    "logreg_model.fit(X_train, y_train)\n",
    "\n",
    "logreg_results = evaluate(logreg_model, \"Logistic Regression + TF-IDF\")\n",
    "logreg_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5474ae35",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_model = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
    "nb_model.fit(X_train, y_train)\n",
    "\n",
    "nb_results = evaluate(nb_model, \"Naive Bayes + TF-IDF\")\n",
    "nb_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47842656",
   "metadata": {},
   "source": [
    "### Final model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c001a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "steam_model = make_pipeline(features, LinearSVC(C=1.0))\n",
    "steam_model.fit(X_train, y_train)\n",
    "\n",
    "svm_results = evaluate(steam_model, \"Final Model: LinearSVC + TF-IDF + Numeric Features\")\n",
    "svm_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae9af4c",
   "metadata": {},
   "source": [
    "### Model comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b249797",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame([\n",
    "    baseline_results,\n",
    "    nb_results,\n",
    "    logreg_results,\n",
    "    svm_results\n",
    "])\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6835de",
   "metadata": {},
   "source": [
    "### Performance plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4cd685",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.set_index(\"model\")[[\"accuracy\",\"f1\"]].plot(kind=\"bar\", figsize=(8,4))\n",
    "\n",
    "plt.title(\"Model Performance Comparison\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e47589",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57d9531",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
