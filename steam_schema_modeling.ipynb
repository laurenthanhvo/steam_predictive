{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMhQlefCilYQ9j+ACyzxcA8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/laurenthanhvo/steam_predictive/blob/casey/steam_schema_modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Steam AU Reviews & Items – Modeling\n",
        "\n",
        "**Context**\n",
        "\n",
        "Context\n",
        "\n",
        "Our predictive task is formulated as a supervised binary classification problem.\n",
        "For each (user, game) interaction, we want to use the user’s review text and gameplay behavior to predict whether the user will recommend the game (recommend = True) or not recommend it (recommend = False).\n",
        "\n",
        "Inputs (Features)\n",
        "\n",
        "We use information available to us in the merged interaction dataset:\n",
        "\n",
        "Gameplay-Based Features\n",
        "\n",
        " -playtime_forever\n",
        "\n",
        "- playtime_2weeks\n",
        "\n",
        "\n",
        "Text-Based Features\n",
        "\n",
        "- TF–IDF vectorization of the review text\n",
        "\n",
        "Game Metadata\n",
        "\n",
        "- Genre\n",
        "\n",
        "Output (Label)\n",
        "\n",
        "- A binary variable:\n",
        "\n",
        "1 → user recommends the game\n",
        "\n",
        "0 → user does not recommend the game\n",
        "\n",
        "Objective Function\n",
        "\n",
        "We optimize the model using standard binary classification objectives:\n",
        "\n",
        "Binary Cross-Entropy Loss (for logistic regression and neural networks)\n",
        "\n",
        "Metrics used for evaluation:\n",
        "\n",
        "- Accuracy\n",
        "\n",
        "- F1 score\n",
        "\n",
        "- Precision / Recall\n",
        "\n",
        "\n",
        "Appropriate Models\n",
        "\n",
        "Because our dataset contains both structured numeric data and unstructured text, multiple model types are suitable:\n",
        "\n",
        "- Logistic Regression (with TF-IDF text features)\n",
        "\n",
        "- Naive Bayes (text-only baseline)\n",
        "\n",
        "- Random Forest or Gradient Boosted Trees (numerical + metadata)\n",
        "\n",
        "- Shallow Neural Network (dense layers on concatenated features)\n",
        "\n",
        "These models allow us to test both simple linear approaches and slightly more complex nonlinear ones.\n",
        "\n",
        "Discussion\n",
        "\n",
        "This section compares modeling approaches and explains why each is useful or limited for this task.\n",
        "\n",
        "1. Logistic Regression\n",
        "\n",
        "Advantages\n",
        "\n",
        "- Strong baseline for text classification\n",
        "\n",
        "- Works well with TF-IDF sparse vectors\n",
        "\n",
        "- Fast to train and easy to interpret\n",
        "\n",
        "- Robust on moderately sized datasets\n",
        "\n",
        "Disadvantages\n",
        "\n",
        "- Linear decision boundary\n",
        "\n",
        "- Struggles with nonlinear interactions between features\n",
        "\n",
        "- Sensitive to feature scaling\n",
        "\n",
        "This model is a standard ML-class baseline and matches course content.\n",
        "\n",
        "2. Naive Bayes\n",
        "\n",
        "Advantages\n",
        "\n",
        "- Extremely fast and lightweight\n",
        "\n",
        "- Performs surprisingly well on raw text\n",
        "\n",
        "- Good baseline for TF-IDF or Bag-of-Words\n",
        "\n",
        "Disadvantages\n",
        "\n",
        "- Assumes independence between features\n",
        "\n",
        "- Not suitable when numeric features matter\n",
        "\n",
        "- Lower accuracy when reviews are long or nuanced\n",
        "\n",
        "We use Naive Bayes as a text-only benchmark to check whether adding gameplay features truly helps.\n",
        "\n",
        "3. Tree-Based Models (Random Forest, XGBoost, LightGBM)\n",
        "\n",
        "Advantages\n",
        "\n",
        "- Capture nonlinear patterns\n",
        "\n",
        "- Handle missing values and skewed numeric data well\n",
        "\n",
        "- Do not require heavy preprocessing\n",
        "\n",
        "Disadvantages\n",
        "\n",
        "- Do not accept high-dimensional TF-IDF matrices directly\n",
        "\n",
        "- Must combine text features via dimensionality reduction or embeddings\n",
        "\n",
        "- Slower to train on large datasets\n",
        "\n",
        "These models help test whether nonlinear relationships in gameplay data matter.\n",
        "\n",
        "4. Neural Networks\n",
        "\n",
        "Advantages\n",
        "\n",
        "- Can combine numeric data with dense text embeddings\n",
        "\n",
        "- More expressive than linear models\n",
        "\n",
        "- Can model interaction effects between playtime and review sentiment\n",
        "\n",
        "Disadvantages\n",
        "\n",
        "- Higher computational cost\n",
        "\n",
        "- Longer training time\n",
        "\n",
        "- Less interpretable\n",
        "\n",
        "- Requires careful tuning to avoid overfitting\n",
        "\n",
        "In this project we use NN only as an optional extension, not as the primary baseline."
      ],
      "metadata": {
        "id": "p7LmSQSzSn1H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "O_nX-LqpRThJ",
        "outputId": "de33d433-4cf6-49d5-a937-356fb48c53a5"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'interactions' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-591028888.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# For now assuming 'interactions' variable already exists:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minteractions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# ----------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'interactions' is not defined"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.dummy import DummyClassifier\n",
        "\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 1. Load your dataset here (adjust path as needed)\n",
        "# ----------------------------------------------------\n",
        "# Example: df = pd.read_csv(\"interactions.csv\")\n",
        "# For now assuming 'interactions' variable already exists:\n",
        "\n",
        "df = interactions.copy()\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 2. Clean and prepare data\n",
        "# ----------------------------------------------------\n",
        "df = df.dropna(subset=[\"review\", \"recommend\"]).copy()\n",
        "df[\"label\"] = df[\"recommend\"].astype(int)\n",
        "df[\"playtime_forever\"] = df[\"playtime_forever\"].fillna(0)\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 3. Train/Test Split\n",
        "# ----------------------------------------------------\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df, df[\"label\"], test_size=0.2, random_state=42, stratify=df[\"label\"]\n",
        ")\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 4. TF-IDF Text Features\n",
        "# ----------------------------------------------------\n",
        "tfidf = TfidfVectorizer(\n",
        "    max_features=20000,\n",
        "    stop_words=\"english\"\n",
        ")\n",
        "\n",
        "X_train_text = tfidf.fit_transform(X_train[\"review\"])\n",
        "X_test_text = tfidf.transform(X_test[\"review\"])\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 5. Numeric Feature (playtime)\n",
        "# ----------------------------------------------------\n",
        "scaler = StandardScaler()\n",
        "\n",
        "X_train_num = scaler.fit_transform(X_train[[\"playtime_forever\"]])\n",
        "X_test_num = scaler.transform(X_test[[\"playtime_forever\"]])\n",
        "\n",
        "# Combine text + numeric\n",
        "X_train_combined = hstack([X_train_text, X_train_num])\n",
        "X_test_combined  = hstack([X_test_text, X_test_num])\n",
        "\n",
        "# ===============================\n",
        "# BASELINES\n",
        "# ===============================\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# Baseline 1: Majority Class\n",
        "# ----------------------------------------------------\n",
        "baseline = DummyClassifier(strategy=\"most_frequent\")\n",
        "baseline.fit(X_train_text, y_train)\n",
        "pred_baseline = baseline.predict(X_test_text)\n",
        "\n",
        "print(\"\\n=== Baseline: Majority Class ===\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, pred_baseline))\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# Baseline 2: Naive Bayes\n",
        "# ----------------------------------------------------\n",
        "nb = MultinomialNB()\n",
        "nb.fit(X_train_text, y_train)\n",
        "pred_nb = nb.predict(X_test_text)\n",
        "\n",
        "print(\"\\n=== Baseline: Naive Bayes ===\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, pred_nb))\n",
        "print(\"F1 Score:\", f1_score(y_test, pred_nb))\n",
        "\n",
        "# ===============================\n",
        "# FINAL MODEL\n",
        "# ===============================\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# Logistic Regression (Text + Numeric Features)\n",
        "# ----------------------------------------------------\n",
        "logreg = LogisticRegression(max_iter=200, n_jobs=-1)\n",
        "logreg.fit(X_train_combined, y_train)\n",
        "\n",
        "pred_lr = logreg.predict(X_test_combined)\n",
        "proba_lr = logreg.predict_proba(X_test_combined)[:, 1]\n",
        "\n",
        "print(\"\\n=== Final Model: Logistic Regression ===\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, pred_lr))\n",
        "print(\"F1 Score:\", f1_score(y_test, pred_lr))\n",
        "print(\"ROC-AUC:\", roc_auc_score(y_test, proba_lr))\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# Optional: Print Most Important Words\n",
        "# ----------------------------------------------------\n",
        "feature_names = np.array(tfidf.get_feature_names_out())\n",
        "coef = logreg.coef_[0][:-1]  # exclude numeric feature\n",
        "\n",
        "top_pos = feature_names[np.argsort(coef)][-15:]\n",
        "top_neg = feature_names[np.argsort(coef)][:15]\n",
        "\n",
        "print(\"\\nTop words predicting recommend=True:\")\n",
        "print(top_pos)\n",
        "\n",
        "print(\"\\nTop words predicting recommend=False:\")\n",
        "print(top_neg)\n"
      ]
    }
  ]
}